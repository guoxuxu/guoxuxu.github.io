---
permalink: /
title: "Xu Guo"
excerpt: "Xu Guo"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I am a Wallenberg-NTU Presidential Postdoctoral Fellow, starting from 2023. I am currently based at <a href="https://www.kth.se/" style="color: #4ca772; text-decoration: none;">KTH, Sweden</a>, working with <a href="https://people.kth.se/~skoglund/" style="color: #4ca772; text-decoration: none;">Mikael Skoglund</a>. I spent the first half of my fellowship at <a href="https://www.ntu.edu.sg/" style="color: #4ca772; text-decoration: none;">NTU, Singapore</a>, where I worked with <a href="https://dr.ntu.edu.sg/cris/rp/rp00084" style="color: #4ca772; text-decoration: none;">Miao Chun Yan</a>. I received my Ph.D. from NTU in 2023, advised by <a href="https://personal.ntu.edu.sg/han.yu/" style="color: #4ca772; text-decoration: none;">Han Yu</a>.

## Research Interests 

My research lies at the intersection of Machine Learning and Natural Language Processing, with a focus on efficient and generative techniques for LLMs.

### Efficient/Low-Resource Methods for NLP
* Data-efficient learning for downstream tasks
* Lightweight model adaptation and training strategies
* Fast and cost-effective inference techniques

### Generation
* Synthetic data generation for downstream tasks
* Data augmentation via controlled generation
* Generating auxiliary signals to support reasoning and learning

<!-- > Research Interests: 1. Natural Language Processing (Pretrained Language Models, Language Understanding, Language Generation, etc.); 2. Machine Learning (Transfer Learning, Adversarial Learning, Federated Learning, etc.).

My PhD research was mostly driven by developing algorithms to adapt (üî•) Pretrained Language Models (PLMs) to low-resource domains against potential domain shift. 
Check out the survey, <a href="https://arxiv.org/pdf/2211.03154.pdf" style="color: #4ca772; text-decoration: none;">on the domain adaptation and generalization of pretrained lanaguage models</a>. 
My thesis, <a href="https://dr.ntu.edu.sg/bitstream/10356/167965/2/PhD_Thesis_GuoXu.pdf" style="color: #4ca772; text-decoration: none;">data-efficient domain adaptation for pretrained language models</a>, 
provides a few promising solutions, such as latent optimization<sup>[[1]](https://aclanthology.org/2021.naacl-main.425.pdf)</sup>, parameter-efficient adaptation<sup>[[2]](https://aclanthology.org/2022.findings-emnlp.258.pdf)</sup> (‚ùÑÔ∏è), 
and personalization<sup>[[3]](https://dl.acm.org/doi/10.1145/3511710)</sup>, to boost PLMs in data-scarce domains under different resource constraints and settings. 

On top of larger PLMs or LLMs for short, my postdoctoral research focus on delivering societal benefits through, e.g., Generative AI<sup>[[4]](https://arxiv.org/pdf/2403.04190)</sup> and Green AI in the real world.
In general, I work on efficient synthetic data generation methods<sup>[[5]](https://dl.acm.org/doi/pdf/10.1145/3581783.3612526),[[6]](https://openreview.net/pdf/22a35aecb8e57c14e0f014a23df9807ab7d1a1e1.pdf)</sup>, 
enhancing the robustness of LLMs<sup>üí™</sup><sup>[[7]](https://arxiv.org/pdf/2406.06633),[[8]](https://arxiv.org/pdf/2407.03993)</sup>, 
accelerating LLM<sup>üöÄ[[9]](https://arxiv.org/pdf/2410.04519)</sup> inference for high throughput, 
and merging LLM for seamless plug-and-play integration.
They are mainly done in a parameter-efficient manner and aim to contribute to our sustainable earth. -->

## Preprints
* <span style="color:rgb(241, 12, 12)">[NEW]</span> **SoftCoT++: Test-Time Scaling with Soft Chain-of-Thought Reasoning**
  <br>_Yige Xu*, Xu Guo*, Zhiwei Zeng, Chunyan Miao_ . [[Paper]](https://arxiv.org/abs/2505.11484)[[Code]](https://github.com/xuyige/SoftCoT)


## Publications üìñ
* **SoftCoT: Soft Chain-of-Thought for Efficient Reasoning with LLMs.**
  <br>_Yige Xu*, Xu Guo*, Zhiwei Zeng, Chunyan Miao_
  <br>ACL, 2025 [[Paper]](https://arxiv.org/abs/2502.12134)[[Code]](https://github.com/xuyige/SoftCoT)[[Data]](https://huggingface.co/datasets/xuyige/ASDiv-Aug)
* **Diffusion-Guided Diversity for Single Domain Generalization in Time Series Classification**
  <br>_Junru Zhang, Lang Feng, Xu Guo, Han Yu, Yabo Dong, Duanqing Xu_
  <br>KDD, 2025 
* **RevMUX: Data Multiplexing with Reversible Adapters for Efficient LLM Batch Inference**
  <br>_Yige Xu, Xu Guo, Zhiwei Zeng, Chunyan Miao_
  <br>EMNLP, 2024 [[Lecture]](https://www.youtube.com/watch?v=ZVZszfrKngE)[[Paper]](https://aclanthology.org/2024.emnlp-main.1232.pdf)[[Code]](https://github.com/xuyige/RevMUX)
* **A Survey on Natural Language Counterfactual Generation**
  <br>_Yongjie Wang*, Xiaoqi Qiu*, Yue Yu, Xu Guo, Zhiwei Zeng, Yuhong Feng, Zhiqi Shen._
  <br>EMNLP Findings, 2024 [[Paper]](https://aclanthology.org/2024.findings-emnlp.276.pdf)
* **Generating Synthetic Datasets for Few-shot Prompt Tuning**
  <br>_Xu Guo, Zilin Du, Boyang Li, Chunyan Miao_
  <br>COLM, 2024 [[Poster]](https://x.com/xuguo_nlp/status/1843294899611009062)[[Paper]](https://openreview.net/pdf/22a35aecb8e57c14e0f014a23df9807ab7d1a1e1.pdf)
* **PairCFR: Enhancing Model Training on Paired Counterfactually Augmented Data through Contrastive Learning**
  <br>_Xiaoqi Qiu*, Yongjie Wang\*, Xu Guo, Zhiwei Zeng, Yue Yu, Yuhong Feng, and Chunyan Miao_
  <br>ACL, 2024 [[Lecture]](https://doi.org/10.48448/dnw2-cq56)[[Paper]](https://aclanthology.org/2024.acl-long.646.pdf)[[Code]](https://github.com/Siki-cloud/PairCFR)
* **InteMATs: Integrating Granularity-Specific Multilingual Adapters for Cross-Lingual Transfer**
  <br>_Meizhen Liu, Xu Guo, Jiakai He, Jianye Chen, Fengyu Zhou, and Siu Hui_
  <br>EMNLP Findings, 2023 [[Paper]](https://aclanthology.org/2023.findings-emnlp.335.pdf)
* **GranCATs: Cross-Lingual Enhancement through Granularity-Specific Contrastive Adapters**
  <br>_Meizhen Liu, Jiakai He, Xu Guo, Jianye Chen, Siu Cheung Hui, and Fengyu Zhou_
  <br>CIKM, 2023 [[Paper]](https://dl.acm.org/doi/10.1145/3583780.3614896)[[Code]](https://github.com/meizhen-nlp/GranCATs)
* **Training Multimedia Event Extraction With Generated Images and Captions**
  <br>_Zilin Du, Yunxin Li, Xu Guo, Yidan Sun, and Boyang Li_
  <br>ACM MM, 2023 [[Paper]](https://dl.acm.org/doi/10.1145/3581783.3612526)[[Code]](https://github.com/ZILIN003/CAMEL)
* **Improving the Sample Efficiency of Prompt Tuning with Domain Adaptation**
  <br>_Xu Guo, Boyang Li, Han Yu_
  <br>EMNLP Findings, 2022 [[Lecture]](https://doi.org/10.48448/5wkx-cp69)[[Paper]](https://aclanthology.org/2022.findings-emnlp.258.pdf)[[Code]](https://github.com/guoxuxu/soft-prompt-transfer/tree/main/optima)
* **Federated learning for personalized humor recognition**
  <br>_Xu Guo, Han Yu, Boyang Li, Hao Wang, Pengwei Xing, Siwei Feng, Zaiqing Nie, and Chunyan Miao_
  <br>ACM TIST, 2022 [[Paper]](https://dl.acm.org/doi/10.1145/3511710)
* **Latent-Optimized Adversarial Neural Transfer for Sarcasm Detection**
  <br>_Xu Guo, Boyang Li, Han Yu, Chunyan Miao_
  <br>NAACL, 2021 [[Lecture]](https://doi.org/10.48448/6j9r-gh59)[[Paper]](https://aclanthology.org/2021.naacl-main.425.pdf)[[Code]](https://github.com/guoxuxu/LOANT)


## Honors and Awards üèÜ
* Wallenberg-NTU Presidential Postdoctoral Fellowship. 2023
* WiEST Development Grant. Women in Engineering, Science, and Technology, NTU. 2023
* Best Presentation Award. Pattern Recognition and Machine Intelligence Association. 2021
* Best Application Paper Award. International Workshop on Federated Learning for User Privacy and Data Confidentiality
in Conjunction with IJCAI 2020
* Student Travel Grant. Awarded by IJCAI. 2019
* NTU Research Scholarship. NTU, Singapore. 2019


----
