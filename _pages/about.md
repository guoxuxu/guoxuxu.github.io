---
permalink: /
title: "Xu Guo"
excerpt: "Xu Guo"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I am a Wallenberg-NTU Presidential Postdoctoral Fellow at <a href="https://www.ntu.edu.sg/" style="color: #4ca772; text-decoration: none;">Nanyang Technological University, Singapore</a> advised by Prof. <a href="https://dr.ntu.edu.sg/cris/rp/rp00084" style="color: #4ca772; text-decoration: none;">Miao Chun Yan</a>.
Previously, I was a research fellow at <a href="https://www.ntu.edu.sg/scse" style="color: #4ca772; text-decoration: none;">SCSE, NTU</a>
under the supervision of Prof. <a href="http://www.boyangli.org/" style="color: #4ca772; text-decoration: none;">Boyang Albert Li</a>.
I received my Ph.D. in 2023 from NTU, where I was supervised by Prof. <a href="https://personal.ntu.edu.sg/han.yu/" style="color: #4ca772; text-decoration: none;">Han Yu</a>.

[//]: # (> <span style="color:red"> *Hiring!* </span> A <a href="https://ntu.wd3.myworkdayjobs.com/en-US/Careers/details/Research-Scientist--Computer-Science-Natural-Language-Processing-_R00017008">Research Scientist</a> and a )

[//]: # (> <a href="https://ntu.wd3.myworkdayjobs.com/en-US/Careers/details/Research-Engineer-I-or-II--Computer-Science--Natural-Language-Processing-_R00017009"> Research Engineer &#40;I or II&#41;</a> position is available at NTU!)

[//]: # (> Candidates will be working on Large Language Models for Health. )


## Research

> Research Interests: 1. Natural Language Processing (Pretrained Language Models, Language Understanding, Language Generation, etc.); 2. Machine Learning (Transfer Learning, Adversarial Learning, Federated Learning, etc.).

My PhD research was mostly driven by developing algorithms to adapt (üî•) Pretrained Language Models (PLMs) to low-resource domains against potential domain shift. 
Check out the survey, <a href="https://arxiv.org/pdf/2211.03154.pdf" style="color: #4ca772; text-decoration: none;">on the domain adaptation and generalization of pretrained lanaguage models</a>. 
My thesis, <a href="https://dr.ntu.edu.sg/bitstream/10356/167965/2/PhD_Thesis_GuoXu.pdf" style="color: #4ca772; text-decoration: none;">data-efficient domain adaptation for pretrained language models</a>, 
provides a few promising solutions, such as latent optimization<sup>[[1]](https://aclanthology.org/2021.naacl-main.425.pdf)</sup>, parameter-efficient adaptation<sup>[[2]](https://aclanthology.org/2022.findings-emnlp.258.pdf)</sup> (‚ùÑÔ∏è), 
and personalization<sup>[[3]](https://dl.acm.org/doi/10.1145/3511710)</sup>, to boost PLMs in data-scarce domains under different resource constraints and settings. 

On top of larger PLMs or LLMs for short, my postdoctoral research focus on delivering societal benefits through, e.g., Generative AI<sup>[[4]](https://arxiv.org/pdf/2403.04190)</sup> and Green AI in the real world.
In general, I work on efficient synthetic data generation methods<sup>[[5]](https://dl.acm.org/doi/pdf/10.1145/3581783.3612526),[[6]](https://openreview.net/pdf/22a35aecb8e57c14e0f014a23df9807ab7d1a1e1.pdf)</sup>, 
enhancing the robustness of LLMs<sup>üí™</sup><sup>[[7]](https://arxiv.org/pdf/2406.06633),[[8]](https://arxiv.org/pdf/2407.03993)</sup>, 
accelerating LLM<sup>üöÄ[[9]](https://arxiv.org/pdf/2410.04519)</sup> inference for high throughput, 
and merging LLM for seamless plug-and-play integration.
They are mainly done in a parameter-efficient manner and aim to contribute to our sustainable earth.


## Publications üìñ
<span style="color: #4ca772">[EMNLP'24]</span> Yige Xu, **Xu Guo**, Zhiwei Zeng, Chunyan Miao. "RevMUX: Data Multiplexing with Reversible Adapters for Efficient LLM Batch Inference" (Long Paper)

<span style="color: #4ca772">[EMNLP Findings'24]</span> Yongjie Wang, Xiaoqi Qiu, Yue Yu, **Xu Guo**, Zhiwei Zeng, Yuhong Feng, Zhiqi Shen. "A Survey on Natural Language Counterfactual Generation" (Long Paper)

<span style="color: #4ca772">[COLM'24]</span> **Xu Guo**, Zilin Du, Boyang Li, Chunyan Miao. "Generating Synthetic Datasets for Few-shot Prompt Tuning" (Long Paper)

<span style="color: #4ca772">[ACL'24]</span> Xiaoqi Qiu\*, Yongjie Wang\*, **Xu Guo**, Zhiwei Zeng, Yue Yu, Yuhong Feng, and Chunyan Miao. "PairCFR: Enhancing Model Training on Paired Counterfactually Augmented Data through Contrastive Learning" (Long Paper)

<span style="color: #4ca772">[EMNLP Findings'23]</span> Meizhen Liu, **Xu Guo**, Jiakai He, Jianye Chen, Fengyu Zhou, and Siu Hui. "InteMATs: Integrating Granularity-Specific Multilingual Adapters for Cross-Lingual Transfer". (Long Paper)

<span style="color: #4ca772">[CIKM'23]</span> Meizhen Liu, Jiakai He, **Xu Guo**, Jianye Chen, Siu Cheung Hui, and Fengyu Zhou. "GranCATs: Cross-Lingual Enhancement through Granularity-Specific Contrastive Adapters". (Long Paper)

<span style="color: #4ca772">[ACM MM'23]</span> Zilin Du, Yunxin Li, **Xu Guo**, Yidan Sun, and Boyang Li. "Training Multimedia Event Extraction With Generated Images and Captions". (Long Paper) 

<span style="color: #4ca772">[IJCNN'22]</span> Fei Luo, Hangwei Qian, Di Wang, **Xu Guo**, Yan Sun, Eng Sing Lee, Hui Hwang Teong, Ray Tian Rui Lai, Chunyan Miao. "Missing value imputation for diabetes prediction". (Long Paper)

<span style="color: #4ca772">[EMNLP Findings'22]</span> **Xu Guo**, Boyang Li, Han Yu. "Improving the Sample Efficiency of Prompt Tuning with Domain Adaptation". (Long Paper)

<span style="color: #4ca772">[ACM TIST'22]</span> **Xu Guo**, Han Yu, Boyang Li, Hao Wang, Pengwei Xing, Siwei Feng, Zaiqing Nie, and Chunyan Miao. "Federated learning for personalized humor recognition" (Long Paper) <span style="color: red">[FL-IJCAI Best Application Paper Award]</span>

<span style="color: #4ca772">[NAACL'21]</span> **Xu Guo**, Boyang Li, Han Yu, Chunyan Miao. "Latent-Optimized Adversarial Neural Transfer for Sarcasm Detection" (Long Paper) <span style="color: red">[PREMIA Best Presentation Award]</span>

<span style="color: #4ca772">[IJCAI'19]</span> **Xu Guo**, Han Yu, Chunyan Miao, and Yiqiang Chen. "Agent-based Decision Support for Pain Management in Primary Care Settings" (Demo Paper)



[//]: # (News)

[//]: # (---)

[//]: # (* 10.23/2023. Served as a Session Chair at CIKM.)

[//]: # (* 10.9/2023. Visiting TUM, Germany.)

[//]: # (* 10/2023. One [paper]&#40;https://aclanthology.org/2023.findings-emnlp.335.pdf&#41; accepted to EMNLP Findings. )

[//]: # (* 08/2023. One [paper]&#40;https://dl.acm.org/doi/pdf/10.1145/3583780.3614896&#41; accepted to CIKM. )

[//]: # (* 07/2023. One [paper]&#40;https://browse.arxiv.org/pdf/2306.08966.pdf&#41; accepted to ACM MM.)

[//]: # (* 06/2023. Awarded [Wallenberg-NTU Presidential Postdoctoral Fellowship.]&#40;https://www.ntu.edu.sg/research/research-careers/presidential-postdoctoral-fellowship-&#40;ppf&#41;#Content_C048_Col01&#41;)

[//]: # (* 02/05/2023. Successfully defended my PhD.)

[//]: # (* 02/2023. Awarded [WiEST Development Grant]&#40;https://www.ntu.edu.sg/women/wiest-development-grant&#41;. Women in STEM at NTU.)


## Fellowship and Grant üèÜ



1. Wallenberg-NTU Presidential Postdoctoral Fellowship. 2023 Dec - 2024 Dec. NTU, Singapore. Principal Investigator - "Advancing Low-resource Regimes with Generative AI", S$100,000.
2. WiEST Development Grant, 2023. Awarded by Women in Engineering, Science, and Technology, NTU. S$3,000
3. Student Travel Grant, 2019. Awared by IJCAI. U\$ 350


----



